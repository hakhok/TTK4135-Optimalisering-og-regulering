
##### Previous page
[[4.1 Model predictive control]]

#qp #mpc 
Linear MPC applies the MPC concept to problems with quadratic objective functions and linear constraints. Hence, a convex QP problem is solved at each time step. We make two changes to the QP problem before starting the algorithm:
- It runs for all times: $t=0,1,...$
- It includes control input changes: $\Delta u_t$ in the objective function.

$$
minf(z)=\sum_{t=0}^{N-1}x^{\top}_{t+1}Q_{t+1}x_{t+1}
+d_{xt+1}x_{t+1}
+\frac{1}{2}u_t^{\top}R_tu_t+d_{ut}+\frac{1}{2}\Delta u_t^{\top}R_{\Delta t}u_t
$$
Subject to:
$$
\begin{align*}
	x_{t+1}&=A_tx_t+B_tu_t \\
	x_0,u_{-1}&=given \\
	x^{low}&\leq x_t\leq x^{high} \\
	u^{low}&\leq u_t\leq u^{high} \\
	-\Delta u^{high}&\leq \Delta u_t\leq \Delta u^{high}
\end{align*}
$$
Where:
$$
\begin{align}
	Q_t &\geq0 \\
	R_t &\geq0 \\
	R_{\Delta t} &\geq0
\end{align}
$$

#algorithm 
### Linear MPC with state feedback
```python
for t in [0,1,2,...]:
	x_t = get_current_state()
	u_t = solve_QP(x_t)
	apply(u_t)
```

#slack_variable
### Slack variables
Sometimes there is no feasible solution to the problem. Then we might soften the constraints, by adding slack variables.

In the objective function, we add the terms:
$$
\rho^{\top}\epsilon+\frac{1}{2}\epsilon^{\top}S\epsilon
$$
And we change the constraint:
$$
x^{low}-\epsilon\leq x_t\leq x^{high}+\epsilon
$$
The slack variable $\epsilon$ and the tuning parameters $\rho$ and $S$ are designed such that all their elements are positive. The slack variable should be non-zero only is the corresponding constraint are violated. This correspond to _exact penalty function_ if $\rho$ is big enough.

### Will the linear MPC controller always be stable?
Stability cannot be guaranteed even if a feasible solution exists for all timesteps. An MPC controller may give rise to a unstable closed loop system.

One way to "guarantee" stability is to add the constraint:
$$
x_N=0
$$
If there are no model errors, this is only a _nominal stability_ result. Since it means that the model and the system is identical.

### Output feedback

#algorithm 
##### Linear MPC with output feedback
```python
for t in [0,1,2,...]:
	x_t = estimate_state_based_on_measured_data()
	u_t = solve_QP(x_t)
	apply(u_t)
```
#### Kalman estimator
#kalman
Equations for estimating the state using the Kalman gain $K_F$:
$$
\begin{align}
	\hat{x}_{t+1} &= A\hat{x}_t+Bu_t+K_F(y_t-\hat{y_t}) \\
	\hat{y}_t &= C\hat{x}_t \\
	\hat{x}_0&=given
\end{align}
$$
![[Pasted image 20230727212759.png]]

##### Adding noise to the model:
$$
\begin{align}
	x_{t+1}&=Ax_t+Bu_t+v_t \\
	y_t &= Cx_t+w_t
\end{align}
$$

### Reference tracking and integral action
![[4.2.4 Reference tracking and integral action]]

##### Next page
[[4.3 Linear Quadratic control]]
